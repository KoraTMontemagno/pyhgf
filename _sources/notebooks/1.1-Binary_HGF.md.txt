---
jupytext:
  formats: ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.15.1
kernelspec:
  display_name: Python 3 (ipykernel)
  language: python
  name: python3
---

(binary_hgf)=
# The binary Hierarchical Gaussian Filter

```{code-cell} ipython3
:tags: [hide-cell]

%%capture
import sys
if 'google.colab' in sys.modules:
    ! pip install pyhgf
```

```{code-cell} ipython3
import jax.numpy as jnp
from pyhgf.model import HGF
from pyhgf import load_data
import seaborn as sns
import matplotlib.pyplot as plt
```

In this notebook, we demonstrate how to create and fit the standard two-level and three-level Hierarchical Gaussian Filters (HGF) for binary inputs. This class share a lot of similarities with its continuous counterpart described in the [next tutorial](continuous_hgf). Here, the difference is that the input node accepts binary data. Binary responses are widely used in decision-making neuroscience, and standard reinforcement learning algorithms like Rescorla-Wagner are tailored to learn outcomes probability under such configuration. Here, by using a Hierarchical Gaussian Filter, we want to be able to learn from the evolution of higher-level volatility, and the parameters that are influencing the strength of the coupling between lower-level nodes with their parents (i.e. $\omega$, or the `evolution rate` of the 1rst and 2nd levels nodes). The binary version of the Hierarchical Gaussian Filter can take the following structures:

```{figure} ../images/binary.png
---
name: binary-hgf
---
The two-level and three-level Hierarchical Gaussian Filter for binary inputs. Note that the first level $X_{1}$ is a value parent for the binary input node $\mathcal{B}$, and has itself a value parent $X_{2}$. A volatility parent is only used in the context of a 3-level HGF. This is a specificity of the binary model.
```

In this example, we will use data from a decision-making task where the outcome probability was manipulated across time, and observe how the binary HGFs can track switches in response probabilities.

+++

## Imports
We import a time series of binary observations from the decision task described in {cite:p}`Iglesias2021`.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
u, _ = load_data("binary")
```

## Fitting the binary HGF with fixed parameters
### The two-level binary Hierarchical Gaussian Filter
#### Create the model

The node structure corresponding to the two-level and three-level Hierarchical Gaussian Filters are automatically generated from `model_type` and `n_levels` using the nodes parameters provided in the dictionaries. Here we are not performing any optimization so those parameters are fixed to reasonable values.

```{note}
The response function used is the binary surprise at each time point ({py:func}`pyhgf.response.binary_surprise`). In other words, at each time point the model try to update its hierarchy to minimize the discrepancy between the expected and real next binary observation. See also [this tutorial](custom_response_function) to see how to create a custom response function.
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
two_levels_hgf = HGF(
    n_levels=2,
    model_type="binary",
    initial_mu={"1": .0, "2": .5},
    initial_pi={"1": .0, "2": 1.0},
    omega={"2": -3.0},
)
```

This function creates an instance of a HGF model automatically parametrized for a two-level binary structure, so we do not have to worry about creating the node structure ourselves. This class also embed function to add new observations and plot results that we are going to use below. We can have a look at the node structure itself using the {ref}`pyhgf.plots.plot_network` function. This function will automatically dray the provided node structure using [Graphviz](https://github.com/xflr6/graphviz).

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
two_levels_hgf.plot_network()
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

#### Add data

```{code-cell} ipython3
# Provide new observations
two_levels_hgf = two_levels_hgf.input_data(input_data=u)
```

#### Plot trajectories

+++ {"editable": true, "slideshow": {"slide_type": ""}}

A Hierarchical Gaussian Filter is acting as a Bayesian filter when presented with new observation, and by running the update equation forward, we can observe the trajectories of the parameters of the node that are being updated after each new observation (i.e. the mean $\mu$ and the precision $\pi$). The `plot_trajectories` function automatically extracts the relevant parameters given the model structure and will plot their evolution together with the input data.

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
two_levels_hgf.plot_trajectories();
```

+++ {"editable": true, "slideshow": {"slide_type": ""}}

#### Surprise
We can see that the surprise will increase when the time series exhibit more unexpected behaviours. The degree to which a given observation is expected will depend on the expected value and volatility in the input node, which is influenced by the values of higher-order nodes. One way to assess model fit is to look at the total binary surprise for each observation. These values can be returned from the fitted model using the `surprise` method:

```{code-cell} ipython3
two_levels_hgf.surprise()
```

```{note}
The surprise of a model under the observation of new data directly depends on the response function that was used. New response functions can be added and provided using different `response_function_parameters` and `response_function` in the {py:func}`pyhgf.model.HGF.surprise` method. The surprise is then defined as the negative log probability of new observations:

$$S(x) = -\log[Pr(x)]$$
```

+++

### The three-level binary Hierarchical Gaussian Filter
#### Create the model
Here, we create a new {py:class}`pyhgf.model.HGF` instance, setting the number of levels to `3`. Note that we are extending the size of the dictionaries accordingly.

```{code-cell} ipython3
three_levels_hgf = HGF(
    n_levels=3,
    model_type="binary",
    initial_mu={"1": .0, "2": .5, "3": 0.},
    initial_pi={"1": .0, "2": 1.0, "3": 1.0},
    omega={"1": None, "2": -3.0, "3": -2.0},
    rho={"1": None, "2": 0.0, "3": 0.0},
    kappas={"1": None, "2": 1.0},
    eta0=0.0,
    eta1=1.0,
    binary_precision=jnp.inf,
)
```

The node structure now includes a volatility parent at the third level.

```{code-cell} ipython3
three_levels_hgf.plot_network()
```

#### Add data

```{code-cell} ipython3
three_levels_hgf = three_levels_hgf.input_data(input_data=u)
```

#### Plot trajectories

```{code-cell} ipython3
three_levels_hgf.plot_trajectories();
```

## Learning parameters with MCMC sampling
In the previous section, we assumed we knew the parameters of the HGF models that were used to filter the input data. This can give us information on how an agent using these values would behave when presented with these inputs. We can also adopt a different perspective and consider that we want to learn these parameters from the data. Here, we are going to set some of the parameters free and use Hamiltonian Monte Carlo methods (NUTS) to sample their probability density.

Because the HGF classes are built on the top of [JAX](https://github.com/google/jax), they are natively differentiable and compatible with optimisation libraries or can be embedded as regular distributions in the context of a Bayesian network. Here, we are using this approach, and we are going to use [PyMC](https://www.pymc.io/welcome.html) to perform this step. PyMC can use any log probability function (here the negative surprise of the model) as a building block for a new distribution by wrapping it in its underlying tensor library [Aesara](https://aesara.readthedocs.io/en/latest/), now forked as [PyTensor](https://pytensor.readthedocs.io/en/latest/). This PyMC-compatible distribution can be found in the {py:obj}`pyhgf.distribution` sub-module.

```{code-cell} ipython3
import pymc as pm
import arviz as az
from pyhgf.distribution import HGFDistribution
from pyhgf.response import first_level_binary_surprise
```

### Two-level model
#### Creating the model

```{code-cell} ipython3
hgf_logp_op = HGFDistribution(
    n_levels=2,
    model_type="binary",
    input_data=[u],
    response_function=first_level_binary_surprise,
)
```

```{note}
The data is being passed to the distribution when the instance is created, so we won't use the `observed` argument in our PyMC model.
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
with pm.Model() as two_levels_binary_hgf:

    # Set a prior over the evolution rate at the second level.
    omega_2 = pm.Uniform("omega_2", -3.5, 0.0)

    # Call the pre-parametrized HGF distribution here.
    # All parameters are set to their default value, except omega_2.
    pm.Potential("hgf_loglike", hgf_logp_op(omega_2=omega_2))
```

#### Visualizing the model

```{code-cell} ipython3
pm.model_to_graphviz(two_levels_binary_hgf)
```

#### Sampling

```{code-cell} ipython3
with two_levels_binary_hgf:
    two_level_hgf_idata = pm.sample(chains=2)
```

```{code-cell} ipython3
az.plot_trace(two_level_hgf_idata, var_names=["omega_2"]);
plt.tight_layout()
```

### Using the learned parameters
To visualize how the model would behave under the most probable values, we average the $\omega_{2}$ samples and use this value in a new model.

```{code-cell} ipython3
omega_2 = az.summary(two_level_hgf_idata)["mean"]["omega_2"]
```

```{code-cell} ipython3
hgf_mcmc = HGF(
    n_levels=2,
    model_type="binary",
    initial_mu={"1": jnp.inf, "2": 0.5},
    initial_pi={"1": 0.0, "2": 1.0},
    omega={"1": jnp.inf, "2": omega_2},
    rho={"1": 0.0, "2": 0.0},
    kappas={"1": 1.0}).input_data(
        input_data=u
    )
```

```{code-cell} ipython3
hgf_mcmc.plot_trajectories();
```

```{code-cell} ipython3
hgf_mcmc.surprise()
```

### Three-level model
#### Creating the model

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
hgf_logp_op = HGFDistribution(
    n_levels=3,
    model_type="binary",
    input_data=[u],
    response_function=first_level_binary_surprise,
)
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
with pm.Model() as three_levels_binary_hgf:

     # Set a prior over the evolution rate at the second and third levels.
    omega_2 = pm.Uniform("omega_2", -4.0, 0.0)
    omega_3 = pm.Normal("omega_3", -11.0, 2)

    # Call the pre-parametrized HGF distribution here.
    # All parameters are set to their default value except omega_2 and omega_3.
    pm.Potential("hgf_loglike", hgf_logp_op(omega_2=omega_2, omega_3=omega_3))
```

#### Visualizing the model

```{code-cell} ipython3
pm.model_to_graphviz(three_levels_binary_hgf)
```

#### Sampling

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---
with three_levels_binary_hgf:
    three_level_hgf_idata = pm.sample(chains=2)
```

```{code-cell} ipython3
az.plot_trace(three_level_hgf_idata, var_names=["omega_2", "omega_3"]);
plt.tight_layout()
```

### Using the learned parameters
To visualize how the model would behave under the most probable values, we average the $\omega_{2}$ samples and use this value in a new model.

```{code-cell} ipython3
omega_2 = az.summary(three_level_hgf_idata)["mean"]["omega_2"]
omega_3 = az.summary(three_level_hgf_idata)["mean"]["omega_3"]
```

```{code-cell} ipython3
hgf_mcmc = HGF(
    n_levels=3,
    model_type="binary",
    initial_mu={"1": jnp.inf, "2": 0.5, "3": 0.0},
    initial_pi={"1": 0.0, "2": 1e4, "3": 1e1},
    omega={"1": jnp.inf, "2": omega_2, "3": omega_3},
    rho={"1": 0.0, "2": 0.0, "3": 0.0},
    kappas={"1": 1.0, "2": 1.0}).input_data(
        input_data=u
    )
```

```{code-cell} ipython3
hgf_mcmc.plot_trajectories();
```

```{code-cell} ipython3
hgf_mcmc.surprise()
```

# System configuration

```{code-cell} ipython3
%load_ext watermark
%watermark -n -u -v -iv -w -p pyhgf,jax,jaxlib
```

```{code-cell} ipython3
---
editable: true
slideshow:
  slide_type: ''
---

```
